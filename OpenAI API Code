#Created by Andreu Artigues
###########################
from datetime import datetime
start_time = datetime.now()
import openai
import numpy as np
from openai import OpenAI


#The API KEY must be first stored in -myenv, as it will always be the same for us
client = OpenAI()

#Audio transcript. We must have downloaded the audio file we wish to translate.
audio_file = open("TEDJuanjo.mp3", "rb")

#Here we have to translate as well into English,as the original speech is in Spanish
transcript = client.audio.translations.create(
  model="whisper-1", 
  file=audio_file
)
clean_transcript=transcript.text

##Chunking. Separate the clean transcript into parts
words = clean_transcript.split(" ")
chunks = np.array_split(words, 3)

######
system = "I am a highly intelligent question answering bot. Therefore, I need you to summarize the text I am going to tell you"


summary_responses = []

for chunk in chunks:
  text = ' '.join(list(chunk))
  response = openai.chat.completions.create(
    model="gpt-3.5-turbo", 
    messages=[
      {"role": "system", "content": system},
      {"role": "user", "content": text}
    ],
    temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.
    top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top Pâ€™s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.
    frequency_penalty=0,
    presence_penalty=1
  )
  response_text=response.choices[0].message.content
  summary_responses.append(response_text)
  
full_summary = ". ".join(summary_responses)  
print(full_summary)

#do your work here
end_time = datetime.now()
print('\nDuration: {}'.format(end_time - start_time))
